{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"PySpark_DataFrame\").master(\"local[2]\").\\\n",
    "config(\"spark.sql.warehouse.dir\",\"file:///E:/input/spark/warehouse\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\importlib\\_bootstrap.py:219: RuntimeWarning: numpy.ufunc size changed, may indicate binary incompatibility. Expected 192 from C header, got 216 from PyObject\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import Tokenizer,HashingTF\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "\n",
    "# 机器学习dataframe格式数据标准栏位名label + features\n",
    "train = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n",
    "\n",
    "test = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n",
    "    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n",
    "    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, -1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, -0.5]))]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=0,regParam=0.01) # 创建一个estimator实例\n",
    "#print(lr.explainParams()) # 参数解释"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = lr.fit(train)\n",
    "paramMap={lr.maxIter:20,lr.regParam:0.1} # 参数字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = lr.fit(train,paramMap) # fit\n",
    "pred = model2.transform(test) # 在spark.ml中使用transform获得结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0,1.5,1.3] 1.0 [0.05668429360932413,0.9433157063906759] 1.0\n",
      "[3.0,2.0,-0.1] 0.0 [0.9232337866421291,0.07676621335787098] 0.0\n",
      "[0.0,2.2,-1.5] 1.0 [0.11040665017928436,0.8895933498207156] 1.0\n"
     ]
    }
   ],
   "source": [
    "#pred = model2.transform(test)\n",
    "result = pred.select(\"features\",\"label\",\"probability\",\"prediction\").collect()\n",
    "for row in result:\n",
    "    print(row.features, row.label,row.probability, row.prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2 = spark.createDataFrame([\n",
    "    (0, \"a b c d e spark\", 1.0),\n",
    "    (1, \"b d\", 0.0),\n",
    "    (2, \"spark f g h\", 1.0),\n",
    "    (3, \"hadoop a\", 0.0),\n",
    "    (4, \"spark mapreduce\", 1.0),\n",
    "    (5, \"hadoop apache\", 0.0),\n",
    "    (6, \"a b c d e spark\", 1.0),\n",
    "    (7, \"hadoop mapreduce\", 0.0),\n",
    "    (8, \"a b c d e spark\", 1.0),\n",
    "    (9, \"c mapreduce\", 0.0),\n",
    "    (10, \"b mapreduce\", 0.0),\n",
    "    (11, \"d mapreduce\", 0.0),\n",
    "    (12, \"a b c d e spark\", 1.0)\n",
    "], [\"id\", \"text\", \"label\"])\n",
    "\n",
    "test2 = spark.createDataFrame([\n",
    "    (4, \"spark i j k\"),\n",
    "    (5, \"l m n\"),\n",
    "    (6, \"spark hadoop spark\"),\n",
    "    (7, \"apache hadoop\")\n",
    "], [\"id\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"text\",outputCol='words')\n",
    "hashtingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=20,regParam=0.01)\n",
    "pipeline = Pipeline(stages=[tokenizer,hashtingTF,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp1 = tokenizer.transform(train2) # 增加了words新栏位，"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0, text='a b c d e spark', label=1.0, words=['a', 'b', 'c', 'd', 'e', 'spark']),\n",
       " Row(id=1, text='b d', label=0.0, words=['b', 'd']),\n",
       " Row(id=2, text='spark f g h', label=1.0, words=['spark', 'f', 'g', 'h']),\n",
       " Row(id=3, text='hadoop a', label=0.0, words=['hadoop', 'a']),\n",
       " Row(id=4, text='spark mapreduce', label=1.0, words=['spark', 'mapreduce']),\n",
       " Row(id=5, text='hadoop apache', label=0.0, words=['hadoop', 'apache']),\n",
       " Row(id=6, text='a b c d e spark', label=1.0, words=['a', 'b', 'c', 'd', 'e', 'spark']),\n",
       " Row(id=7, text='hadoop mapreduce', label=0.0, words=['hadoop', 'mapreduce']),\n",
       " Row(id=8, text='a b c d e spark', label=1.0, words=['a', 'b', 'c', 'd', 'e', 'spark']),\n",
       " Row(id=9, text='c mapreduce', label=0.0, words=['c', 'mapreduce']),\n",
       " Row(id=10, text='b mapreduce', label=0.0, words=['b', 'mapreduce']),\n",
       " Row(id=11, text='d mapreduce', label=0.0, words=['d', 'mapreduce']),\n",
       " Row(id=12, text='a b c d e spark', label=1.0, words=['a', 'b', 'c', 'd', 'e', 'spark'])]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Row(id=0, text='a b c d e spark', label=1.0, words=['a', 'b', 'c', 'd', 'e', 'spark'], features=SparseVector(262144, {17222: 1.0, 27526: 1.0, 28698: 1.0, 30913: 1.0, 227410: 1.0, 234657: 1.0})),\n",
       " [Row(id=0, text='a b c d e spark', label=1.0, words=['a', 'b', 'c', 'd', 'e', 'spark'], features=SparseVector(262144, {17222: 1.0, 27526: 1.0, 28698: 1.0, 30913: 1.0, 227410: 1.0, 234657: 1.0})),\n",
       "  Row(id=1, text='b d', label=0.0, words=['b', 'd'], features=SparseVector(262144, {27526: 1.0, 30913: 1.0})),\n",
       "  Row(id=2, text='spark f g h', label=1.0, words=['spark', 'f', 'g', 'h'], features=SparseVector(262144, {15554: 1.0, 24152: 1.0, 51505: 1.0, 234657: 1.0})),\n",
       "  Row(id=3, text='hadoop a', label=0.0, words=['hadoop', 'a'], features=SparseVector(262144, {155117: 1.0, 227410: 1.0})),\n",
       "  Row(id=4, text='spark mapreduce', label=1.0, words=['spark', 'mapreduce'], features=SparseVector(262144, {42633: 1.0, 234657: 1.0})),\n",
       "  Row(id=5, text='hadoop apache', label=0.0, words=['hadoop', 'apache'], features=SparseVector(262144, {66695: 1.0, 155117: 1.0})),\n",
       "  Row(id=6, text='a b c d e spark', label=1.0, words=['a', 'b', 'c', 'd', 'e', 'spark'], features=SparseVector(262144, {17222: 1.0, 27526: 1.0, 28698: 1.0, 30913: 1.0, 227410: 1.0, 234657: 1.0})),\n",
       "  Row(id=7, text='hadoop mapreduce', label=0.0, words=['hadoop', 'mapreduce'], features=SparseVector(262144, {42633: 1.0, 155117: 1.0})),\n",
       "  Row(id=8, text='a b c d e spark', label=1.0, words=['a', 'b', 'c', 'd', 'e', 'spark'], features=SparseVector(262144, {17222: 1.0, 27526: 1.0, 28698: 1.0, 30913: 1.0, 227410: 1.0, 234657: 1.0})),\n",
       "  Row(id=9, text='c mapreduce', label=0.0, words=['c', 'mapreduce'], features=SparseVector(262144, {28698: 1.0, 42633: 1.0})),\n",
       "  Row(id=10, text='b mapreduce', label=0.0, words=['b', 'mapreduce'], features=SparseVector(262144, {30913: 1.0, 42633: 1.0})),\n",
       "  Row(id=11, text='d mapreduce', label=0.0, words=['d', 'mapreduce'], features=SparseVector(262144, {27526: 1.0, 42633: 1.0})),\n",
       "  Row(id=12, text='a b c d e spark', label=1.0, words=['a', 'b', 'c', 'd', 'e', 'spark'], features=SparseVector(262144, {17222: 1.0, 27526: 1.0, 28698: 1.0, 30913: 1.0, 227410: 1.0, 234657: 1.0}))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp2 = HashingTF(inputCol='words', outputCol=\"features\").transform(tmp1) #增加了特征新栏位\n",
    "tmp2.collect()[0],tmp2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[id: bigint, text: string, label: double, words: array<string>, features: vector]\n"
     ]
    }
   ],
   "source": [
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=4, text='spark i j k', probability=DenseVector([0.1089, 0.8911]), prediction=1.0),\n",
       " Row(id=5, text='l m n', probability=DenseVector([0.9541, 0.0459]), prediction=0.0),\n",
       " Row(id=6, text='spark hadoop spark', probability=DenseVector([0.0033, 0.9967]), prediction=1.0),\n",
       " Row(id=7, text='apache hadoop', probability=DenseVector([0.9943, 0.0057]), prediction=0.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = pipeline.fit(train2)\n",
    "pred = model.transform(test2)\n",
    "selected = pred.select('id','text','probability','prediction')\n",
    "selected.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import Tokenizer,HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder,CrossValidator,TrainValidationSplit\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#管道\n",
    "tokenizer = Tokenizer(inputCol=\"text\",outputCol='words')\n",
    "hashtingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "#lr = LogisticRegression(maxIter=20,regParam=0.01)\n",
    "lr = LogisticRegression(maxIter=20)\n",
    "pipeline = Pipeline(stages=[tokenizer,hashtingTF,lr])\n",
    "\n",
    "#参数网格\n",
    "paramGrid = ParamGridBuilder().addGrid(lr.elasticNetParam,[0.1, 0.5, 0.7, 0.9]).build()\n",
    "#效果评估\n",
    "binEvaluator = BinaryClassificationEvaluator(labelCol=\"label\", \n",
    "                    rawPredictionCol=\"prediction\",metricName=\"areaUnderROC\")\n",
    "\n",
    "# 模型选择/网格搜索\n",
    "tvs = TrainValidationSplit(estimator=pipeline,estimatorParamMaps=paramGrid,\n",
    "                           evaluator=binEvaluator,trainRatio=0.9)\n",
    "#模型训练\n",
    "model =tvs.fit(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5,\n",
       "  {Param(parent='LogisticRegression_41b6b49e9630bcedf44f', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1}),\n",
       " (0.5,\n",
       "  {Param(parent='LogisticRegression_41b6b49e9630bcedf44f', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5}),\n",
       " (0.5,\n",
       "  {Param(parent='LogisticRegression_41b6b49e9630bcedf44f', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.7}),\n",
       " (0.5,\n",
       "  {Param(parent='LogisticRegression_41b6b49e9630bcedf44f', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.9})]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型训练的评估\n",
    "model.validationMetrics # 对应参数的得分，\n",
    "#如果是cross交叉训练\n",
    "#model.avgMetrics\n",
    "#list(zip(model.validationMetrics, paramGrid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: bigint, text: string, label: double, words: array<string>, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 模型预测\n",
    "preds = model.transform(train2)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='LogisticRegression_41b6b49e9630bcedf44f', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.1},\n",
       " {Param(parent='LogisticRegression_41b6b49e9630bcedf44f', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.5},\n",
       " {Param(parent='LogisticRegression_41b6b49e9630bcedf44f', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.7},\n",
       " {Param(parent='LogisticRegression_41b6b49e9630bcedf44f', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.9}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
