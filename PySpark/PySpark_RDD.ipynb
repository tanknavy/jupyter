{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext,SparkConf\n",
    "#from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"PySpark01\").setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_encryption_enabled',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_serialize_to_jvm',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobDescription',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 3712)\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socketserver.py\", line 720, in __init__\n",
      "    self.handle()\n",
      "  File \"C:\\software\\spark-2.3.3-bin-hadoop2.7\\python\\pyspark\\accumulators.py\", line 270, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"C:\\software\\spark-2.3.3-bin-hadoop2.7\\python\\pyspark\\accumulators.py\", line 242, in poll\n",
      "    if func():\n",
      "  File \"C:\\software\\spark-2.3.3-bin-hadoop2.7\\python\\pyspark\\accumulators.py\", line 246, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"C:\\software\\spark-2.3.3-bin-hadoop2.7\\python\\pyspark\\serializers.py\", line 690, in read_int\n",
      "    length = stream.read(4)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\socket.py\", line 589, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "ConnectionResetError: [WinError 10054] 远程主机强迫关闭了一个现有的连接。\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用parallelize从现有集合中产生rdd\n",
    "data = list(range(10))\n",
    "rdd1 = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_computeFractionForSampleSize',\n",
       " '_defaultReducePartitions',\n",
       " '_id',\n",
       " '_jrdd',\n",
       " '_jrdd_deserializer',\n",
       " '_memory_limit',\n",
       " '_pickled',\n",
       " '_reserialize',\n",
       " '_to_java_object_rdd',\n",
       " 'aggregate',\n",
       " 'aggregateByKey',\n",
       " 'cache',\n",
       " 'cartesian',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'cogroup',\n",
       " 'collect',\n",
       " 'collectAsMap',\n",
       " 'combineByKey',\n",
       " 'context',\n",
       " 'count',\n",
       " 'countApprox',\n",
       " 'countApproxDistinct',\n",
       " 'countByKey',\n",
       " 'countByValue',\n",
       " 'ctx',\n",
       " 'distinct',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatMap',\n",
       " 'flatMapValues',\n",
       " 'fold',\n",
       " 'foldByKey',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'fullOuterJoin',\n",
       " 'getCheckpointFile',\n",
       " 'getNumPartitions',\n",
       " 'getStorageLevel',\n",
       " 'glom',\n",
       " 'groupBy',\n",
       " 'groupByKey',\n",
       " 'groupWith',\n",
       " 'histogram',\n",
       " 'id',\n",
       " 'intersection',\n",
       " 'isCheckpointed',\n",
       " 'isEmpty',\n",
       " 'isLocallyCheckpointed',\n",
       " 'is_cached',\n",
       " 'is_checkpointed',\n",
       " 'join',\n",
       " 'keyBy',\n",
       " 'keys',\n",
       " 'leftOuterJoin',\n",
       " 'localCheckpoint',\n",
       " 'lookup',\n",
       " 'map',\n",
       " 'mapPartitions',\n",
       " 'mapPartitionsWithIndex',\n",
       " 'mapPartitionsWithSplit',\n",
       " 'mapValues',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'meanApprox',\n",
       " 'min',\n",
       " 'name',\n",
       " 'partitionBy',\n",
       " 'partitioner',\n",
       " 'persist',\n",
       " 'pipe',\n",
       " 'randomSplit',\n",
       " 'reduce',\n",
       " 'reduceByKey',\n",
       " 'reduceByKeyLocally',\n",
       " 'repartition',\n",
       " 'repartitionAndSortWithinPartitions',\n",
       " 'rightOuterJoin',\n",
       " 'sample',\n",
       " 'sampleByKey',\n",
       " 'sampleStdev',\n",
       " 'sampleVariance',\n",
       " 'saveAsHadoopDataset',\n",
       " 'saveAsHadoopFile',\n",
       " 'saveAsNewAPIHadoopDataset',\n",
       " 'saveAsNewAPIHadoopFile',\n",
       " 'saveAsPickleFile',\n",
       " 'saveAsSequenceFile',\n",
       " 'saveAsTextFile',\n",
       " 'setName',\n",
       " 'sortBy',\n",
       " 'sortByKey',\n",
       " 'stats',\n",
       " 'stdev',\n",
       " 'subtract',\n",
       " 'subtractByKey',\n",
       " 'sum',\n",
       " 'sumApprox',\n",
       " 'take',\n",
       " 'takeOrdered',\n",
       " 'takeSample',\n",
       " 'toDebugString',\n",
       " 'toLocalIterator',\n",
       " 'top',\n",
       " 'treeAggregate',\n",
       " 'treeReduce',\n",
       " 'union',\n",
       " 'unpersist',\n",
       " 'values',\n",
       " 'variance',\n",
       " 'zip',\n",
       " 'zipWithIndex',\n",
       " 'zipWithUniqueId']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(rdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外部数据源：local file, HDFS, Cassandra,Hbase, S3,包括textFile, SequenceFiles等\n",
    "# 在HDFS上默认一个block(128mb)一个partition\n",
    "rdd2 = sc.textFile(\"file:///E:input/text/p*.txt\") # 每个文件一行一条记录\n",
    "rdd2a = sc.wholeTextFiles(\"file:///E:input/text/*txt\") # pairs[filename, content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd2.count()\n",
    "#rdd2.coalesce(1).saveAsPickleFile(\"file:///E:input/text/pickle/rdd2\")\n",
    "rdd2.getNumPartitions()\n",
    "rdd2_pickle = sc.pickleFile(\"file:///E:input/text/pickle/rdd2/part-00000\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name:Mercury\\\\r\\\\ndescription:Mercury is the god of commerce...\\\\r\\\\ntype:rocky planet', 'name:Venus\\\\r\\\\ndescription:Venus is the goddess of love...\\\\r\\\\ntype:rocky planet']\n"
     ]
    }
   ],
   "source": [
    "print(rdd2_pickle.collect()[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.coalesce(1).saveAsPickleFile(\"file:///E:input/text/pickle/rdd2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "输入文件格式\n",
    "SequenceFiles: flat,binary file type\n",
    "Parqute:\n",
    "ORC\n",
    "JSON\n",
    "Hive tables\n",
    "JDBC\n",
    "Avro\n",
    "Apache Arrow:\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 键值对\n",
    "lines = sc.textFile(\"file:///E:input/text/test.txt\")\n",
    "import re\n",
    "#words = lines.flatMap(lambda x : x.split(r\"\\W+\"))\n",
    "words = lines.flatMap(lambda x : re.split(\"\\W+\", x))\n",
    "word_pairs = words.map(lambda x:(x,1))\n",
    "counts = word_pairs.reduceByKey(lambda a,b:a+b)\n",
    "counts.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spark转换Transformations:\n",
    "map(func)\n",
    "filter(func)\n",
    "flatMap(func)\n",
    "mapPartitions(func):类似map,但是按照每个partion(block),func:iterator<T> =>iterator<U>\n",
    "mapPartitionsWithIndex(func):类似上面带有partition的索引，func: (Int,iterator<T> =>iterator<U>)\n",
    "sample(withReleacement, fraction,seed): 随机采样\n",
    "union(otherDataset): 类似sql的union联合两个dataset的元素，不排除重复值\n",
    "intersection(otherDataset): 返回两个dataset的交集\n",
    "distinct(numPartitions): 只包含唯一元素的dataset\n",
    "groupByKey(numPartitions): (k,v) =>(k,iterable<v>), 可以设定任务数量\n",
    "reduceByKey(func,[numPartitions]): (k,v) => (k,v),func必须是(v,v) =>v, reduce任务的个数可以设定\n",
    "aggregateByKey(zeroValue)(seqOp,combOp,[numPartitions]): 输入输出的类型不一样(k,v) =>(k,u)，否则用reduceByKey\n",
    "sortByKey([ascending],[numPartitions]): (k,v)对中k实现Ordered类\n",
    "sortBy: 全局有序的rdd\n",
    "join(otherDataset,[numPartitions]): (K,V).join(K,W) =>(K,(V,W)),leftOuterJoin,rightOuterJoin,fullOuterJoin.\n",
    "cogroup(otherDataset,[numPartitions]): (K,V).join(K,W) =>(K,(Iterable<V>,Iterable<W>)) \n",
    "cartesian(otherDataset): 笛卡尔积\n",
    "pipe(command,[envVars]):rdd的每个partition通过一个shell命令\n",
    "coalesce(numPartitions):降低partions到指定数量\n",
    "repartition(numPartitions): 跨过全局重新洗牌创建更多或更少的partition，并且全局balance\n",
    "repartitionAnsSortWithPartitions(patitioner): 参考partitioner重新分区,在每一个partition内排序，比reparition效率更高\n",
    "        \n",
    "'''\n",
    "aggregateByKey(zeroValue)(seqOp,combOp,[numPartitions])总结：\n",
    "zeroValue：可以为0如果是求和，\" \"如果是集合类型, Double.MinValue如果求最大,Doublemax.MaxValue如果求最小，key值不用写\n",
    "SeqOp转换/合并在同一个partition中的数据，在写func时，key值不用写\n",
    "CombOp合并不同partition中已经聚集好的数据，在写func时，key值不用写\n",
    "\n",
    "zero_val = (0,0)\n",
    "def seq_op(accumulator,element):\n",
    "def comb_op(accumulator1,accumulator2):\n",
    "# https://backtobazics.com/big-data/spark/apache-spark-aggregatebykey-example/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Spark动作Actions\n",
    "reduce(func): 使用func聚集数据，func要满足交换律和结合律\n",
    "collect(): 返回全部元素作为一个array到driver程序端，\n",
    "count(): 返回元素个数\n",
    "first(): 返回第一个元素\n",
    "take(n): 返回前n个元素的array\n",
    "takeSample(withReplacement, num,seed): 返回随机采样的元素集合作为一个array\n",
    "takeOrdered(n,[ordering]): 返回前n个元素按照自然排序或者自定义排序\n",
    "\n",
    "saveAsTextFile(path):每个元素调用toString转换，text file格式写到文件系统路径\n",
    "savaAsSequenceFile(path):仅限java/scala，写dataset的元素作为hadoop的SequenceFile, rdd的k/v对实现hadoop的Writable接口\n",
    "saveAsObjectFile(path): 使用java serialization写每个元素\n",
    "countByKey():在(k,v)对的rdd上返回hashmap格式的(k,Int)对\n",
    "\n",
    "foreach(func): func作用于每个元素，\n",
    "foreachAsync\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "RDD持久化：\n",
    "MEMEORY_ONLY, MEMORY_AND_DISK,\n",
    "MEMORY_ONLY_SER: 仅限java/scala, 储存rdd像一个序列化的java object\n",
    "MEMORY_AND_DISK_SER\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Shared Variables\n",
    "\n",
    "Broadcast广播变量\n",
    "broadcast_var = sc.broadcast([1,2,3]) #\n",
    "\n",
    "Accumulators累加变量\n",
    "accum = sc.accumulator(0) # Accumulator<id=0,value=0>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "d1 = list(range(15))\n",
    "d2 = list(range(10,25))\n",
    "random.shuffle(d1),random.shuffle(d2) #原地洗牌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1,d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = sc.parallelize(d1)\n",
    "r2 = sc.parallelize(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1.getNumPartitions(),r2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_part(partition): # 输入是每一个partition\n",
    "    rdd = [] #输出必须为iterator<U>\n",
    "    for x in partition:\n",
    "        rdd.append((x,1))\n",
    "    return rdd # 返回一个Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1a = r1.map(lambda x: (x,1)).sortByKey(True,5)\n",
    "r2a = r2.mapPartitions(func_part).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = r1a.intersection(r2a).collect()\n",
    "r4 = r1a.cogroup(r2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r4a =r4.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(r4a[10][1][1])\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
