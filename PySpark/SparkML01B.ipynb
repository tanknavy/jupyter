{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[2]\",\"Spark with Python\")\n",
    "user_data = sc.textFile(\"C:/input/spark/ml-100k/u.user\")\n",
    "user_data.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "user_fields = user_data.map(lambda line: line.split(\"|\"))\n",
    "user_fields.take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_users = user_fields.map(lambda fields: fields[0]).count()\n",
    "num_genders = user_fields.map(lambda fields:fields[2]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_occupations = user_fields.map(lambda fields:fields[3]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_zipcodes = user_fields.map(lambda fields:fields[4]).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 943, genders: 2,occupations: 21, zip_codes: 795 \n"
     ]
    }
   ],
   "source": [
    "print(\"User: %d, genders: %d,occupations: %d, zip_codes: %d \" %(num_users,num_genders,num_occupations,num_zipcodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ac32\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n"
     ]
    }
   ],
   "source": [
    "ages = user_fields.map(lambda x:int(x[1])).collect()\n",
    "plt.hist(ages,bins=20,color='lightblue',normed=True)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,10)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '24', 'M', 'technician', '85711'],\n",
       " ['2', '53', 'F', 'other', '94043'],\n",
       " ['3', '23', 'M', 'writer', '32067'],\n",
       " ['4', '24', 'M', 'technician', '43537'],\n",
       " ['5', '33', 'F', 'other', '15213']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_by_occupation = user_fields.map(lambda fields:(fields[3],1)).reduceByKey(lambda x,y:x+y).collect()\n",
    "user_fields.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_axis1 = np.array([c[0] for c in count_by_occupation])\n",
    "y_axis1 = np.array([c[1] for c in count_by_occupation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_axis = x_axis1[np.argsort(y_axis1)]\n",
    "y_axis = y_axis1[np.argsort(y_axis1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ac32\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:98: MatplotlibDeprecationWarning: \n",
      "Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  \"Adding an axes using the same arguments as a previous axes \"\n"
     ]
    }
   ],
   "source": [
    "pos = np.arange(len(x_axis))\n",
    "width = 1.0\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(pos + (width /2))\n",
    "ax.set_xticklabels(x_axis)\n",
    "\n",
    "plt.bar(pos,y_axis,width,color='lightblue')\n",
    "plt.xticks(rotation=30)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,10)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.defaultdict'>\n",
      "Map-reduce   approach: {'writer': 45, 'doctor': 7, 'educator': 95, 'lawyer': 12, 'technician': 27, 'artist': 28, 'executive': 32, 'engineer': 67, 'administrator': 79, 'student': 196, 'scientist': 31, 'marketing': 26, 'healthcare': 16, 'other': 105, 'retired': 14, 'librarian': 51, 'salesman': 12, 'entertainment': 18, 'homemaker': 7, 'programmer': 66, 'none': 9}\n",
      "\n",
      "countByValue approach: {'lawyer': 12, 'technician': 27, 'executive': 32, 'none': 9, 'administrator': 79, 'student': 196, 'marketing': 26, 'scientist': 31, 'healthcare': 16, 'writer': 45, 'homemaker': 7, 'educator': 95, 'other': 105, 'artist': 28, 'engineer': 67, 'retired': 14, 'librarian': 51, 'salesman': 12, 'entertainment': 18, 'doctor': 7, 'programmer': 66}\n"
     ]
    }
   ],
   "source": [
    "count_by_occupation2 = user_fields.map(lambda fields:fields[3]).countByValue()\n",
    "print(type(count_by_occupation2))\n",
    "print(\"Map-reduce   approach:\", dict(count_by_occupation))\n",
    "print(\"\")\n",
    "print(\"countByValue approach:\", dict(count_by_occupation2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0',\n",
       " '2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0',\n",
       " '3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0',\n",
       " '4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0',\n",
       " '5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_data = sc.textFile(\"C:/input/spark/ml-100k/u.item\")\n",
    "movie_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_movies = movie_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movies: 1682\n"
     ]
    }
   ],
   "source": [
    "print(\"movies: %d\" % num_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_year(x):\n",
    "    try:\n",
    "        return int(x[-4:])\n",
    "    except:\n",
    "        return 1900 # 若是据缺失年份则默认设定为1900，后续处理中过滤掉这类数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    }
   ],
   "source": [
    "movie_fields = movie_data.map(lambda lines: lines.split(\"|\"))\n",
    "#movie_fields.take(5)\n",
    "print(type(movie_fields))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1995, 1995, 1995, 1995, 1995]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x))\n",
    "print(type(years))\n",
    "years.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1995, 1995, 1995, 1995, 1995]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years_filtered = years.filter(lambda x: x!=1900)\n",
    "years_filtered.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.defaultdict'>\n",
      "defaultdict(<class 'int'>, {0: 65, 1: 286, 2: 355, 3: 219, 4: 214, 5: 126, 6: 37, 7: 22, 8: 24, 9: 15, 10: 11, 11: 13, 12: 15, 13: 7, 14: 8, 15: 5, 16: 13, 17: 12, 18: 8, 19: 9, 20: 4, 21: 4, 22: 5, 23: 6, 24: 8, 25: 4, 26: 3, 27: 7, 28: 3, 29: 4, 30: 6, 31: 5, 32: 2, 33: 5, 34: 2, 35: 6, 36: 5, 37: 3, 38: 5, 39: 4, 40: 9, 41: 8, 42: 4, 43: 5, 44: 7, 45: 2, 46: 3, 47: 5, 48: 7, 49: 4, 50: 3, 51: 5, 52: 5, 53: 4, 54: 5, 55: 4, 56: 2, 57: 5, 58: 8, 59: 7, 60: 3, 61: 4, 62: 2, 63: 4, 64: 4, 65: 2, 66: 1, 67: 1, 68: 1, 72: 1, 76: 1})\n",
      "dict_values([65, 286, 355, 219, 214, 126, 37, 22, 24, 15, 11, 13, 15, 7, 8, 5, 13, 12, 8, 9, 4, 4, 5, 6, 8, 4, 3, 7, 3, 4, 6, 5, 2, 5, 2, 6, 5, 3, 5, 4, 9, 8, 4, 5, 7, 2, 3, 5, 7, 4, 3, 5, 5, 4, 5, 4, 2, 5, 8, 7, 3, 4, 2, 4, 4, 2, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "movie_ages = years_filtered.map(lambda yr: 1998-yr).countByValue()\n",
    "print(type(movie_ages))\n",
    "print(movie_ages)\n",
    "values = movie_ages.values()\n",
    "print(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 72, 76]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ac32\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n"
     ]
    }
   ],
   "source": [
    "bins = list(movie_ages.keys()) # dict_keys-->list\n",
    "print(type(bins),bins)\n",
    "plt.hist(values, bins=bins, color='lightblue', normed=True)\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(16,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n",
      "196\t242\t3\t881250949\n"
     ]
    }
   ],
   "source": [
    "rating_data_raw = sc.textFile(\"C:/input/spark/ml-100k/u.data\")\n",
    "print(type(rating_data_raw))\n",
    "print(rating_data_raw.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_ratings: 100000\n"
     ]
    }
   ],
   "source": [
    "num_ratings = rating_data_raw.count()\n",
    "print(\"num_ratings: %d\" %num_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'> [3, 3, 1, 2, 1]\n",
      "<class 'list'> [3, 3, 1, 2, 1] 4.0\n"
     ]
    }
   ],
   "source": [
    "rating_data = rating_data_raw.map(lambda line:line.split(\"\\t\"))\n",
    "ratings = rating_data.map(lambda fields:int(fields[2]))\n",
    "print(type(ratings),ratings.take(5))\n",
    "max_rating = ratings.reduce(lambda x,y:max(x,y))\n",
    "min_rating = ratings.reduce(lambda x,y:min(x,y))\n",
    "mean_rating = ratings.reduce(lambda x,y:x+y) / num_ratings #均值\n",
    "median_rating = np.median(ratings.collect()) # numpy求中位数\n",
    "print(type(ratings.collect()),ratings.collect()[:5],median_rating,)\n",
    "ratings_per_user = num_ratings / num_users\n",
    "ratings_per_movie = num_ratings / num_movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rating: 3.53\n"
     ]
    }
   ],
   "source": [
    "print (\"Average rating: %2.2f\" % mean_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count: 100000, mean: 3.529859999999947, stdev: 1.1256679707622548, max: 5.0, min: 1.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> <class 'numpy.ndarray'> 5 <class 'numpy.ndarray'>\n",
      "[ 6110. 11370. 27145. 34174. 21201.] dict_values([6110, 11370, 27145, 34174, 21201])\n"
     ]
    }
   ],
   "source": [
    "count_by_rating = ratings.countByValue()\n",
    "x_axis = list(count_by_rating.keys())\n",
    "y_axis = np.array([ float(c) for c in count_by_rating.values()]) # 为了方便后学的y值求和，\n",
    "y_axis2 = np.array(count_by_rating.values()) # 数据类型外面相等，里面不相等\n",
    "print(type(x_axis),type(y_axis),len(y_axis),type(y_axis2))\n",
    "print(y_axis,y_axis2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ac32\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matplotlib\\figure.py:98: MatplotlibDeprecationWarning: \n",
      "Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n",
      "  \"Adding an axes using the same arguments as a previous axes \"\n"
     ]
    }
   ],
   "source": [
    "y_axis_normed = y_axis / y_axis.sum() # 对y轴正则化，使它表示百分比\n",
    "pos = np.arange(len(x_axis))\n",
    "width = 1.0\n",
    "ax = plt.axes()\n",
    "ax.set_xticks(pos + (width / 2))\n",
    "ax.set_xticklabels(x_axis)\n",
    "plt.bar(pos, y_axis_normed, width, color='lightblue')\n",
    "#plt.xticks(rotation=30)\n",
    "fig = plt.gcf() # Get a reference to the current figure\n",
    "fig.set_size_inches(16, 10)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, <pyspark.resultiterable.ResultIterable at 0x1721ea90>),\n",
       " (2, <pyspark.resultiterable.ResultIterable at 0x1721ef98>),\n",
       " (3, <pyspark.resultiterable.ResultIterable at 0x1721ed68>),\n",
       " (4, <pyspark.resultiterable.ResultIterable at 0x1721efd0>),\n",
       " (5, <pyspark.resultiterable.ResultIterable at 0x17221400>)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings_grouped = rating_data.map(lambda fields: (int(fields[0]),\n",
    "int(fields[2]))).groupByKey()\n",
    "user_ratings_grouped.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 272), (2, 62), (3, 54), (4, 24), (5, 175)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_ratings_byuser = user_ratings_grouped.map(lambda kv: (kv[0], len(kv[1]))) #新语法\n",
    "user_ratings_byuser.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[272, 62, 54, 24, 175]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ac32\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py:6521: MatplotlibDeprecationWarning: \n",
      "The 'normed' kwarg was deprecated in Matplotlib 2.1 and will be removed in 3.1. Use 'density' instead.\n",
      "  alternative=\"'density'\", removal=\"3.1\")\n"
     ]
    }
   ],
   "source": [
    "user_ratings_byuser_local = user_ratings_byuser.map(lambda kv:kv[1]).collect()\n",
    "print(user_ratings_byuser_local[:5])\n",
    "plt.hist(user_ratings_byuser_local,bins=200,color='lightblue',normed=True)\n",
    "fig = plt.gcf() # gcf = Get a reference to Current Figure\n",
    "fig.set_size_inches(16,10)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> [1995 1995 1995 1995 1995]\n",
      "[array([ True,  True,  True, ...,  True,  True,  True])] 1658\n"
     ]
    }
   ],
   "source": [
    "years_pre_processed = movie_fields.map(lambda fields:fields[2])\\\n",
    ".map(lambda x:convert_year(x)).collect()\n",
    "years_pre_processed_array = np.array(years_pre_processed)\n",
    "print(type(years_pre_processed_array), years_pre_processed_array[:5])\n",
    "print([years_pre_processed_array != 1990][:5],np.sum([years_pre_processed_array != 1990]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.int64'> 266\n"
     ]
    }
   ],
   "source": [
    "mean_year = np.mean(years_pre_processed_array[years_pre_processed_array != 1990])\n",
    "median_year = np.median(years_pre_processed_array[years_pre_processed_array != 1990])\n",
    "index_bad_data = np.where(years_pre_processed_array==1900)[0][0] # 等于的第一个index\n",
    "print(type(index_bad_data),index_bad_data)\n",
    "years_pre_processed_array[index_bad_data] = median_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([], dtype=int64),) 1682\n",
      "1995\n"
     ]
    }
   ],
   "source": [
    "print(np.where(years_pre_processed_array==1900),len(years_pre_processed_array))\n",
    "print(years_pre_processed_array[266])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['administrator', 'artist', 'doctor', 'educator', 'engineer', 'entertainment', 'executive', 'healthcare', 'homemaker', 'lawyer', 'librarian', 'marketing', 'none', 'other', 'programmer', 'retired', 'salesman', 'scientist', 'student', 'technician', 'writer']\n"
     ]
    }
   ],
   "source": [
    "all_occupations = user_fields.map(lambda fields:fields[3]).distinct().collect()\n",
    "print(type(all_occupations))\n",
    "all_occupations.sort()\n",
    "print(all_occupations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "Encoding of 'doctor:' 2\n"
     ]
    }
   ],
   "source": [
    "idx = 0\n",
    "all_occupations_dict = {}\n",
    "for o in all_occupations:\n",
    "    all_occupations_dict[o] = idx\n",
    "    idx += 1\n",
    "print(type(all_occupations_dict))\n",
    "print(\"Encoding of 'doctor:' %d\" % all_occupations_dict['doctor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binrary feature vector: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.] \n"
     ]
    }
   ],
   "source": [
    "K = len(all_occupations_dict)\n",
    "binary_x = np.zeros(K)\n",
    "k_programmer = all_occupations_dict['programmer']\n",
    "binary_x[k_programmer] = 1\n",
    "print(\"Binrary feature vector: %s \" %binary_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PythonRDD[60] at RDD at PythonRDD.scala:48 [7, 12, 23, 21, 21]\n"
     ]
    }
   ],
   "source": [
    "def extract_datetime(ts):\n",
    "    import datetime\n",
    "    return datetime.datetime.fromtimestamp(ts)\n",
    "\n",
    "timestamps = rating_data.map(lambda fields:int(fields[3]))\n",
    "hour_of_day = timestamps.map(lambda ts:extract_datetime(ts).hour)\n",
    "print(timestamps,hour_of_day.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['morning', 'lunch', None, 'evening', 'evening']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def assgin_tod(hr):\n",
    "    times_of_day = {\n",
    "        'morning' : range(7,12),\n",
    "        'lunch' : range(12,14),\n",
    "        'afternoon': range(14,18),\n",
    "        'evening': range(18,23),\n",
    "        'night': range(23,7)\n",
    "    }\n",
    "    for k,v in times_of_day.items():\n",
    "        if hr in v:\n",
    "            return k\n",
    "      \n",
    "times_of_day = {\n",
    "        'morning' : range(7,12),\n",
    "        'lunch' : range(12,14),\n",
    "        'afternoon': range(14,18),\n",
    "        'evening': range(18,23),\n",
    "        'night': range(23,7)}\n",
    "\n",
    "\n",
    "times_of_day = hour_of_day.map(lambda hr:assgin_tod(hr))\n",
    "times_of_day.take(5)\n",
    "#for k,v in times_of_day.items():\n",
    "#    if 8 in v:\n",
    "#        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 提取简单的文本特征\n",
    "def extract_title(raw):\n",
    "    import re\n",
    "    #该表达式找寻括号之间的非单词（数字）\n",
    "    # search扫描整个字符串并返回第一个成功的匹配。否者返回None\n",
    "    grps = re.search(\"\\((\\w+)\\)\", raw) # 等价于'[A-Za-z0-9_]',寻找‘(字符)’\n",
    "    if grps:\n",
    "        return raw[:grps.start()].strip() # 只选取标题部分，并删除末尾的空白字符,start匹配位置的开始\n",
    "    else:\n",
    "        return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'Toy Story (1995)', '01-Jan-1995', '', 'http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)', '0', '0', '0', '1', '1', '1', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0']]\n",
      "Toy Story\n",
      "GoldenEye\n",
      "Four Rooms\n",
      "Get Shorty\n",
      "Copycat\n"
     ]
    }
   ],
   "source": [
    "print(movie_fields.take(1))\n",
    "raw_titles = movie_fields.map(lambda fields:fields[1])\n",
    "for raw_title in raw_titles.take(5):\n",
    "    print(extract_title(raw_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'> [['Toy', 'Story'], ['GoldenEye'], ['Four', 'Rooms'], ['Get', 'Shorty'], ['Copycat']]\n"
     ]
    }
   ],
   "source": [
    "movie_titles = raw_titles.map(lambda m: extract_title(m))\n",
    "title_terms = movie_titles.map(lambda t: t.split(\" \")) # 接下来两种方法\n",
    "print(type(title_terms),title_terms.take(5))\n",
    "# 更多处理细节，大小写转换，删除标点符合和特殊字符，删除停用词，词干提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['', 'Butcher', 'Femme', 'Das', 'Just']\n",
      "['Butcher', 'Femme', 'Das', 'Just', 'Indian']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "#all_terms = title_terms.flatMap(lambda x: x).distinct().collect()\n",
    "all_terms = title_terms.flatMap(lambda x: x).map(lambda x: re.sub('\\W+','',x)).distinct().collect() #去掉特殊字符\n",
    "\n",
    "#all_terms_cleaned = all_terms.map(lambda x: re.sub('\\W+','',x)) # 匹配非字母字符，即匹配特殊字符，去掉特殊字符\n",
    "print(type(all_terms))\n",
    "print(all_terms[:5]) # 发现第一个元素是空值''\n",
    "all_terms.remove(all_terms[0]) # 前面collect从rdd转换成list,现已出第一个元素''\n",
    "print(all_terms[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of terms: 2425\n",
      "Index of term 'Dead': 2004\n",
      "Index of term 'Rooms': 238\n"
     ]
    }
   ],
   "source": [
    "# 创建一个新的字典保存词，并分配k之1序号\n",
    "idx = 0\n",
    "all_terms_dict = {} #字典\n",
    "for term in all_terms:\n",
    "    all_terms_dict[term] = idx #注意是【】\n",
    "    idx += 1\n",
    "print(\"Total number of terms: %d\" % len(all_terms_dict))\n",
    "print(\"Index of term 'Dead': %d\" % all_terms_dict['Dead'])\n",
    "print(\"Index of term 'Rooms': %d\" % all_terms_dict['Rooms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "# 也可以通过spark的zipWithIndex函数更高效得到相同的结果，该函数以各值的RDD为输入，\n",
    "# 对值进行合并以生成一个新的键值对RDD\n",
    "all_terms_dict2 = title_terms.flatMap(lambda x: x).map(lambda x: re.sub('\\W+','',x)).distinct().\\\n",
    "zipWithIndex().collectAsMap() # \\换行，zipWithIndex()编码，\n",
    "print(type(all_terms_dict2))\n",
    "#print(\"Index of term 'Dead': %d\" % all_terms_dict2['Dead'])\n",
    "#print(\"Index of term 'Rooms': %d\" % all_terms_dict2['Rooms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.PipelinedRDD'> [['Toy', 'Story'], ['GoldenEye'], ['Four', 'Rooms'], ['Get', 'Shorty'], ['Copycat']]\n",
      "<class 'pyspark.broadcast.Broadcast'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<1x2425 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 2 stored elements in Compressed Sparse Column format>,\n",
       " <1x2425 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1 stored elements in Compressed Sparse Column format>,\n",
       " <1x2425 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 2 stored elements in Compressed Sparse Column format>,\n",
       " <1x2425 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 2 stored elements in Compressed Sparse Column format>,\n",
       " <1x2425 sparse matrix of type '<class 'numpy.float64'>'\n",
       " \twith 1 stored elements in Compressed Sparse Column format>]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个函数，将词集合转换为一个scipy稀疏向量\n",
    "def create_vector(terms,term_dict):\n",
    "    from scipy import sparse as sp\n",
    "    num_terms = len(term_dict)\n",
    "    x = sp.csc_matrix((1,num_terms))\n",
    "    for t in terms:\n",
    "        if t in term_dict:\n",
    "            idx = term_dict[t]\n",
    "            x[0,idx] =1\n",
    "    return x\n",
    "\n",
    "all_terms_bcast = sc.broadcast(all_terms_dict)\n",
    "print(type(title_terms),title_terms.take(5))\n",
    "print(type(all_terms_bcast))\n",
    "term_vectors = title_terms.map(lambda terms:create_vector(terms,all_terms_bcast.value)) # 广播变量.value变为dict\n",
    "term_vectors.take(5) # 每个电影标题都被转换为一个稀疏向量，有几个词对应几个非零元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:\n",
      " [ 0.25739993 -0.90848143 -0.37850311 -0.5349156   0.85807335 -0.41300998\n",
      "  0.49818858  2.01019925  1.26286154 -0.43921486]\n",
      "2-norm of x: 2.8818\n",
      "normalized x: \n",
      " [ 0.08931963 -0.31524962 -0.13134331 -0.18561957  0.29775765 -0.14331745\n",
      "  0.17287504  0.69755365  0.43822207 -0.15241073] \n",
      "2-Norm of normalized_x: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 正则化特征(column)和正则化特征向量(row)\n",
    "np.random.seed(43)\n",
    "x = np.random.randn(10) # 随机正态分布10个数\n",
    "norm_x_2 = np.linalg.norm(x) # 默认2范数\n",
    "normalized_x = x / norm_x_2\n",
    "\n",
    "print(\"x:\\n %s\" %x)\n",
    "print(\"2-norm of x: %2.4f\" % norm_x_2)\n",
    "print(\"normalized x: \\n %s \" % normalized_x)\n",
    "print(\"2-Norm of normalized_x: %2.4f\" % np.linalg.norm(normalized_x)) # 手动正则化以后求范数，应该为1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Normalized x MLlib:\n",
      "[ 0.08931963 -0.31524962 -0.13134331 -0.18561957  0.29775765 -0.14331745\n",
      "  0.17287504  0.69755365  0.43822207 -0.15241073]\n",
      "2-Norm of normalized x MLlib:\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# MLLib正则化特征\n",
    "from pyspark.mllib.feature import Normalizer\n",
    "normalizer = Normalizer() # 初始化Normalizer,其默认使用与之前相同的二阶范数\n",
    "# Distribute a local Scala collection to form an RDD\n",
    "vector = sc.parallelize([x]) # 输入为一个RDD（它包含numpy数值或MLlib向量）\n",
    "\n",
    "normalized_x_mllib = normalizer.transform(vector).first().toArray() #取rdd第一个值再转换为numpy数组\n",
    "print(type(normalized_x_mllib))\n",
    "print(\"Normalized x MLlib:\\n%s\" % normalized_x_mllib)\n",
    "print(\"2-Norm of normalized x MLlib:\\n%s\" % np.linalg.norm(normalized_x_mllib))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'特征提取可借助的软件包有scikit-learn、gensim、scikit-image、matplotlib、Python\\n的NLTK、Java编写的OpenNLP以及用Scala编写的Breeze\\n和Chalk。实际上，Breeze自Spark 1.0开始就成为Spark的一部分了。\\n'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''特征提取可借助的软件包有scikit-learn、gensim、scikit-image、matplotlib、Python\n",
    "的NLTK、Java编写的OpenNLP以及用Scala编写的Breeze\n",
    "和Chalk。实际上，Breeze自Spark 1.0开始就成为Spark的一部分了。\n",
    "'''\n",
    "# 如何导入、处理和清理数据，以及如何将原始数据转为特征向量以供模型训练的常见方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
