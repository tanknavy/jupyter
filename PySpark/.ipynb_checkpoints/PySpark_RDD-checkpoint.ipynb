{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext,SparkConf\n",
    "#from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName(\"PySpark01\").setMaster(\"local[2]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用parallelize从现有集合中产生rdd\n",
    "data = list(range(10))\n",
    "rdd1 = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 外部数据源：local file, HDFS, Cassandra,Hbase, S3,包括textFile, SequenceFiles等\n",
    "# 在HDFS上默认一个block(128mb)一个partition\n",
    "rdd2 = sc.textFile(\"file:///E:input/text/p*.txt\") # 每个文件一行一条记录\n",
    "rdd2a = sc.wholeTextFiles(\"file:///E:input/text/*txt\") # pairs[filename, content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdd2.count()\n",
    "#rdd2.coalesce(1).saveAsPickleFile(\"file:///E:input/text/pickle/rdd2\")\n",
    "rdd2.getNumPartitions()\n",
    "rdd2_pickle = sc.pickleFile(\"file:///E:input/text/pickle/rdd2/part-00000\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['name:Mercury\\\\r\\\\ndescription:Mercury is the god of commerce...\\\\r\\\\ntype:rocky planet', 'name:Venus\\\\r\\\\ndescription:Venus is the goddess of love...\\\\r\\\\ntype:rocky planet']\n"
     ]
    }
   ],
   "source": [
    "print(rdd2_pickle.collect()[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2.coalesce(1).saveAsPickleFile(\"file:///E:input/text/pickle/rdd2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "输入文件格式\n",
    "SequenceFiles: flat,binary file type\n",
    "roc\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 1),\n",
       " ('python', 3),\n",
       " ('java', 3),\n",
       " ('scala', 2),\n",
       " ('', 4),\n",
       " ('Do', 1),\n",
       " ('yes', 1),\n",
       " ('I', 2),\n",
       " ('javascript', 2),\n",
       " ('you', 1),\n",
       " ('program', 2),\n",
       " ('with', 2),\n",
       " ('and', 1),\n",
       " ('every', 2),\n",
       " ('day', 2),\n",
       " ('did', 1)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 键值对\n",
    "lines = sc.textFile(\"file:///E:input/text/test.txt\")\n",
    "import re\n",
    "#words = lines.flatMap(lambda x : x.split(r\"\\W+\"))\n",
    "words = lines.flatMap(lambda x : re.split(\"\\W+\", x))\n",
    "word_pairs = words.map(lambda x:(x,1))\n",
    "counts = word_pairs.reduceByKey(lambda a,b:a+b)\n",
    "counts.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-73-645bae516169>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-73-645bae516169>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    \"\u001b[0m\n\u001b[1;37m     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Spark转换Transformations:\n",
    "map(func)\n",
    "filter(func)\n",
    "flatMap(func)\n",
    "mapPartitions(func):类似map,但是按照每个partion(block),func:iterator<T> =>iterator<U>\n",
    "mapPartitionsWithIndex(func):类似上面带有partition的索引，func: (Int,iterator<T> =>iterator<U>)\n",
    "sample(withReleacement, fraction,seed): 随机采样\n",
    "union(otherDataset): 类似sql的union联合两个dataset的元素，不排除重复值\n",
    "intersection(otherDataset): 返回两个dataset的交集\n",
    "distinct(numPartitions): 只包含唯一元素的dataset\n",
    "groupByKey(numPartitions): (k,v) =>(k,iterable<v>), 可以设定任务数量\n",
    "reduceByKey(func,[numPartitions]): (k,v) => (k,v),func必须是(v,v) =>v, reduce任务的个数可以设定\n",
    "aggregateByKey(zeroValue)(seqOp,combOp,[numPartitions]): 输入输出的类型不一样(k,v) =>(k,u)，否则用reduceByKey\n",
    "sortByKey([ascending],[numPartitions]): (k,v)对中k实现Ordered类\n",
    "sortBy: 全局有序的rdd\n",
    "join(otherDataset,[numPartitions]): (K,V).join(K,W) =>(K,(V,W)),leftOuterJoin,rightOuterJoin,fullOuterJoin.\n",
    "cogroup(otherDataset,[numPartitions]): (K,V).join(K,W) =>(K,(Iterable<V>,Iterable<W>)) \n",
    "cartesian(otherDataset): 笛卡尔积\n",
    "pipe(command,[envVars]):rdd的每个partition通过一个shell命令\n",
    "coalesce(numPartitions):降低partions到指定数量\n",
    "repartition(numPartitions): 跨过全局重新洗牌创建更多或更少的partition，并且全局balance\n",
    "repartitionAnsSortWithPartitions(patitioner): 参考partitioner重新分区,在每一个partition内排序，比reparition效率更高\n",
    "        \n",
    "'''\n",
    "aggregateByKey(zeroValue)(seqOp,combOp,[numPartitions])总结：\n",
    "zeroValue：可以为0如果是求和，\" \"如果是集合类型, Double.MinValue如果求最大,Doublemax.MaxValue如果求最小，key值不用写\n",
    "SeqOp转换/合并在同一个partition中的数据，在写func时，key值不用写\n",
    "CombOp合并不同partition中已经聚集好的数据，在写func时，key值不用写\n",
    "\n",
    "zero_val = (0,0)\n",
    "def seq_op(accumulator,element):\n",
    "def comb_op(accumulator1,accumulator2):\n",
    "# https://backtobazics.com/big-data/spark/apache-spark-aggregatebykey-example/\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSpark动作Actions\\nreduce(func): 使用func聚集数据，func要满足交换律和结合律\\ncollect(): 返回全部元素作为一个array到driver程序端，\\ncount(): 返回元素个数\\nfirst(): 返回第一个元素\\ntake(n): 返回前n个元素的array\\ntakeSample(withReplacement, num,seed): 返回随机采样的元素集合作为一个array\\ntakeOrdered(n,[ordering]): 返回前n个元素按照自然排序或者自定义排序\\n\\nsaveAsTextFile(path):每个元素调用toString转换，text file格式写到文件系统路径\\nsavaAsSequenceFile(path):仅限java/scala，写dataset的元素作为hadoop的SequenceFile, rdd的k/v对实现hadoop的Writable接口\\nsaveAsObjectFile(path): 使用java serialization写每个元素\\ncountByKey():在(k,v)对的rdd上返回hashmap格式的(k,Int)对\\n\\nforeach(func): func作用于每个元素，\\nforeachAsync\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Spark动作Actions\n",
    "reduce(func): 使用func聚集数据，func要满足交换律和结合律\n",
    "collect(): 返回全部元素作为一个array到driver程序端，\n",
    "count(): 返回元素个数\n",
    "first(): 返回第一个元素\n",
    "take(n): 返回前n个元素的array\n",
    "takeSample(withReplacement, num,seed): 返回随机采样的元素集合作为一个array\n",
    "takeOrdered(n,[ordering]): 返回前n个元素按照自然排序或者自定义排序\n",
    "\n",
    "saveAsTextFile(path):每个元素调用toString转换，text file格式写到文件系统路径\n",
    "savaAsSequenceFile(path):仅限java/scala，写dataset的元素作为hadoop的SequenceFile, rdd的k/v对实现hadoop的Writable接口\n",
    "saveAsObjectFile(path): 使用java serialization写每个元素\n",
    "countByKey():在(k,v)对的rdd上返回hashmap格式的(k,Int)对\n",
    "\n",
    "foreach(func): func作用于每个元素，\n",
    "foreachAsync\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nRDD持久化：\\nmemeroy_only,memery_and_disk,\\nmemery_only_ser: 储存rdd像一个序列化的java object\\n'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "RDD持久化：\n",
    "MEMEORY_ONLY, MEMORY_AND_DISK,\n",
    "MEMORY_ONLY_SER: 仅限java/scala, 储存rdd像一个序列化的java object\n",
    "MEMORY_AND_DISK_SER\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nShared Variables\\n\\nBroadcast广播变量\\nbroadcast_var = sc.broadcast([1,2,3]) #\\n\\nAccumulators累加变量\\naccum = sc.accumulator(0) # Accumulator<id=0,value=0>\\n\\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Shared Variables\n",
    "\n",
    "Broadcast广播变量\n",
    "broadcast_var = sc.broadcast([1,2,3]) #\n",
    "\n",
    "Accumulators累加变量\n",
    "accum = sc.accumulator(0) # Accumulator<id=0,value=0>\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "d1 = list(range(15))\n",
    "d2 = list(range(10,25))\n",
    "random.shuffle(d1),random.shuffle(d2) #原地洗牌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6, 4, 13, 10, 5, 3, 11, 14, 0, 7, 1, 8, 9, 12, 2],\n",
       " [10, 14, 24, 17, 11, 13, 12, 22, 23, 20, 21, 18, 16, 19, 15])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1,d2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = sc.parallelize(d1)\n",
    "r2 = sc.parallelize(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1.getNumPartitions(),r2.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_part(partition): # 输入是每一个partition\n",
    "    rdd = [] #输出必须为iterator<U>\n",
    "    for x in partition:\n",
    "        rdd.append((x,1))\n",
    "    return rdd # 返回一个Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1a = r1.map(lambda x: (x,1)).sortByKey(True,5)\n",
    "r2a = r2.mapPartitions(func_part).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = r1a.intersection(r2a).collect()\n",
    "r4 = r1a.cogroup(r2a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "r4a =r4.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "it = iter(r4a[10][1][1])\n",
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-bc1ab118995a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
